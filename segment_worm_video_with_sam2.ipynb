{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Segmenting *C. elegans* in videos with SAM 2\n",
    "\n",
    "This notebook shows how to use SAM 2 for interactive segmentation of worms in videos (adjusted from video_predictor_example from SAM2 repository). It will cover the following:\n",
    "\n",
    "- adding clicks on a frame to get and refine _masklets_ (spatio-temporal masks)\n",
    "- propagating clicks to get _masklets_ throughout the video\n",
    "- segmenting and tracking multiple objects at the same time\n",
    "\n",
    "We use the terms _segment_ or _mask_ to refer to the model prediction for an object on a single frame, and _masklet_ to refer to the spatio-temporal masks across the entire video."
   ],
   "metadata": {
    "id": "9F_LmdDCfIJi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment Set-up"
   ],
   "metadata": {
    "id": "rYCwJD27gR9o"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If running locally using jupyter, first install `sam2` in your environment using the [installation instructions](https://github.com/facebookresearch/sam2#installation) in the repository.\n",
    "\n",
    "If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'. Note that it's recommended to use **A100 or L4 GPUs when running in Colab** (T4 GPUs might also work, but could be slow and might run out of memory in some cases).\n",
    "\n",
    "It is recommended to use Google Colab, unless you have powerful hardware loacally!"
   ],
   "metadata": {
    "id": "5_0_klqugUKA"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<a href=\"https://colab.research.google.com/github/pwetterauer/Notebooks/blob/main/segment_worm_video.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
  },
  {
   "cell_type": "code",
   "source": [
    "using_colab = True"
   ],
   "metadata": {
    "id": "x25H__8lf8cH"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install opencv-python matplotlib\n",
    "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam2.git'\n",
    "\n",
    "    !mkdir -p ../checkpoints/\n",
    "    !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt"
   ],
   "metadata": {
    "id": "bEr_CaCKg3r3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set-up\n"
   ],
   "metadata": {
    "id": "YnbwUXaLg_57"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ],
   "metadata": {
    "id": "2sh9ELvhg-cL"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ],
   "metadata": {
    "id": "BApJb7kPhHq2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the SAM 2 video predictor\n"
   ],
   "metadata": {
    "id": "mP0Ad2MQhO57"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "sam2_checkpoint = \"../checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"
   ],
   "metadata": {
    "id": "A5eZ3acchSEC"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)"
   ],
   "metadata": {
    "id": "fAfmvqaJh3fq"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Select an example video\n"
   ],
   "metadata": {
    "id": "pGsJ7jwmh9BV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We assume that the video is stored as a list of JPEG frames with filenames like `<frame_index>.jpg`.\n",
    "\n",
    "You can extract their JPEG frames using ffmpeg (https://ffmpeg.org/) as follows:\n",
    "```\n",
    "ffmpeg -i <your_video>.mp4 -q:v 2 -start_number 0 <output_dir>/'%05d.jpg'\n",
    "```\n",
    "where `-q:v` generates high-quality JPEG frames (higher number -> lower quality) and `-start_number 0` asks ffmpeg to start the JPEG file from `00000.jpg`.\n",
    "\n",
    "Upload the videos to your Google Drive, into a directory called 'video' or adjust the path to the frames in code. Mount your drive to Google Colab by running the cell below.\n"
   ],
   "metadata": {
    "id": "X0-lKFlzh_tm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "ZWXdwxXnEWLU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# `video_dir` a directory of JPEG frames with filenames like `<frame_index>.jpg`\n",
    "video_dir = \"../content/drive/MyDrive/video\"\n",
    "\n",
    "# scan all the JPEG frame names in this directory\n",
    "frame_names = [\n",
    "    p for p in os.listdir(video_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "# take a look the first video frame\n",
    "frame_idx = 0\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))"
   ],
   "metadata": {
    "id": "Mo0YlOibkgZm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Initialize the inference state\n"
   ],
   "metadata": {
    "id": "pir5lKmsnRoq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SAM 2 requires stateful inference for interactive video segmentation, so we need to initialize an **inference state** on this video.\n",
    "\n",
    "During initialization, it loads all the JPEG frames in `video_path` and stores their pixels in `inference_state` (as shown in the progress bar below).\n"
   ],
   "metadata": {
    "id": "DHxcC_N7nTm9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "inference_state = predictor.init_state(video_path=video_dir)"
   ],
   "metadata": {
    "id": "iA-YRZFYnY_O"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Track several worms at once"
   ],
   "metadata": {
    "id": "hIzYSIJLniYv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add position data for all worms. Use e.g. ImageJ to find coordinates of points inside each worm you want to track.\n",
    "\n",
    "`points` should contain one array for each worm with coordinates for points. There should be at least one point per worm, but you can add more. You can also add points outside the worm ('negative points') to better define the boundarys. Each point should be given as `[x-coordinate, y-coordinate]`.\n",
    "\n",
    "`labels` should contain one array per worm with 1 for 'positive points' and 0 for 'negative points'.\n",
    "\n",
    "Adjust `nr_of_worms` accordingly!"
   ],
   "metadata": {
    "id": "RJHBS-p-nm_m"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# this example contains data for 4 worms in frame 0. For each worm two 'positive points' and one 'negative point' arte given.\n",
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "nr_of_worms = 4    # number of worms\n",
    "worm_points = np.array([[[318, 154], [312, 170], [318, 167]],\n",
    "                   [[276, 294], [286, 322], [279, 316]],\n",
    "                   [[415, 300], [419, 325], [426, 324]],\n",
    "                   [[578, 237], [596, 266], [593, 252]]],dtype=np.float32)\n",
    "worm_labels = np.array([[1,1,0],\n",
    "                   [1,1,0],\n",
    "                   [1,1,0],\n",
    "                   [1,1,0]],np.int32)\n",
    "\n",
    "if worm_points.shape[0]==nr_of_worms:\n",
    "    print(\"Data ok!\")\n",
    "else:\n",
    "    print(\"'points' should contain one array for each worm! Check the number of worms and added points.\")"
   ],
   "metadata": {
    "id": "8DZ5zp8upVov"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "prompts = {}  # hold all the data we add for visualization\n",
    "\n",
    "for i in range(nr_of_worms):\n",
    "    ann_obj_id = i+1  # give a unique id to each object we interact with\n",
    "\n",
    "    # Adds the points for one worm\n",
    "    points = worm_points[i]\n",
    "    # Adds the corresponding labels\n",
    "    labels = worm_labels[i]\n",
    "    prompts[ann_obj_id] = points, labels\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=ann_frame_idx,\n",
    "        obj_id=ann_obj_id,\n",
    "        points=points,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "# show the results on the current (interacted) frame\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "show_points(points, labels, plt.gca())\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    show_points(*prompts[out_obj_id], plt.gca())\n",
    "    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"
   ],
   "metadata": {
    "id": "n-6DZRJ_tABo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Propagate predicted masks to whole video\n"
   ],
   "metadata": {
    "id": "o5xWYLROt1Xi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the results look good for the one frame, let's propagate them the whole vodeo. This will take some time."
   ],
   "metadata": {
    "id": "hWJbIwFit8VD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "vis_frame_stride = 30\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {out_frame_idx}\")\n",
    "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ],
   "metadata": {
    "id": "rG0eoTPRuSft"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the results"
   ],
   "metadata": {
    "id": "G5fQ4kFhuZ99"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The segmentation masks can be stored in a `results.pk` file. Additionally, the figures visualising the result in single frames can be saved."
   ],
   "metadata": {
    "id": "Pp16s_-NwIYa"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pickle"
   ],
   "metadata": {
    "id": "mlZqLVETwi-r"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with open('/content/drive/MyDrive/result.pk', 'wb') as f:\n",
    "    pickle.dump(video_segments, f)"
   ],
   "metadata": {
    "id": "gScUvgaCwUXC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# shows and saves frame 450 with overlayed masks as png file\n",
    "frame_nr = 450\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "ax.set_title(f\"frame {frame_nr}\")\n",
    "ax.set_axis_off()\n",
    "ax.imshow(Image.open(os.path.join(video_dir, frame_names[frame_nr])))\n",
    "for out_obj_id, out_mask in video_segments[frame_nr].items():\n",
    "    show_mask(out_mask, plt.gca(), obj_id=out_obj_id)\n",
    "fig.savefig(\"../content/drive/MyDrive/frame_with_masks.png\")"
   ],
   "metadata": {
    "id": "gRpmNQ9FxvJH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the saved results"
   ],
   "metadata": {
    "id": "2sk7a4zL78S0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the following cell to load pickled results back into `video_segments` variable."
   ],
   "metadata": {
    "id": "byKxI17f8CEr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "with open('/content/drive/MyDrive/result.pk', 'rb') as f:\n",
    "  video_segments=pickle.load(f)"
   ],
   "metadata": {
    "id": "lQEASM0H7cbp"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
